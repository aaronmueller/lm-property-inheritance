{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "\n",
    "import lexicon\n",
    "import config\n",
    "import utils\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "from semantic_memory import taxonomy, vsm, vsm_utils\n",
    "from nltk.corpus import wordnet as wn\n",
    "from ordered_set import OrderedSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dupes(L):\n",
    "#     seen = set()\n",
    "#     seen2 = set()\n",
    "#     seen_add = seen.add\n",
    "#     seen2_add = seen2.add\n",
    "#     for item in L:\n",
    "#         if item in seen:\n",
    "#             seen2_add(item)\n",
    "#         else:\n",
    "#             seen_add(item)\n",
    "#     return list(seen2)\n",
    "\n",
    "# glove_vocab = []\n",
    "# with open(\"../../glove.vocab\") as f:\n",
    "#     for line in f:\n",
    "#         glove_vocab.append(line.strip(\"\\n\"))\n",
    "\n",
    "# # len(set(glove_vocab))\n",
    "# # get_dupes(glove_vocab)\n",
    "# glove.embeddings.shape, len(set(glove_vocab))\n",
    "\n",
    "# glove = vsm.VectorSpaceModel(\"glove\", 300)\n",
    "# glove.load_vectors(\"../../glove.840B.300d.txt\")\n",
    "\n",
    "# len(glove.vocab2idx)\n",
    "\n",
    "\n",
    "# glove_queries = []\n",
    "# glove_reduced_vocab = []\n",
    "# all_concepts = concept_universe.union(set(anchor_children.keys()))\n",
    "# for c in all_concepts:\n",
    "#     if c in glove.vocab:\n",
    "#         # pass\n",
    "#         glove_queries.append(glove(c))\n",
    "#         glove_reduced_vocab.append(c)\n",
    "#     else:\n",
    "#         try:\n",
    "#             glove_queries.append(glove(c.split(\" \")).mean(0).reshape(1, -1))\n",
    "#             glove_reduced_vocab.append(c)\n",
    "#         except:\n",
    "#             print(c)\n",
    "#             pass\n",
    "\n",
    "# glove_queries = torch.stack(glove_queries, 1).squeeze(0)\n",
    "\n",
    "# len(glove_queries), len(glove_reduced_vocab)\n",
    "\n",
    "# glove_reduced = vsm.VectorSpaceModel(\"glove_queries\")\n",
    "# glove_reduced.load_vectors_from_tensor(glove_queries, glove_reduced_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117659it [00:56, 2080.35it/s]\n"
     ]
    }
   ],
   "source": [
    "VECTORS=\"../../lmms-sp-wsd.albert-xxlarge-v2.synsets.vectors.txt\"\n",
    "\n",
    "sense_embeddings = vsm.VectorSpaceModel(\"LMMS-ALBERT\")\n",
    "# takes about a min to load...\n",
    "sense_embeddings.load_vectors(VECTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tabora.n.01', 0.7659215927124023),\n",
       " ('hargeisa.n.01', 0.7564389705657959),\n",
       " ('cameroon.n.01', 0.7562030553817749),\n",
       " ('gafsa.n.01', 0.7523672580718994),\n",
       " ('huainaputina.n.01', 0.7497690916061401),\n",
       " ('mbeya.n.01', 0.749377965927124),\n",
       " ('kananga.n.01', 0.7490277290344238),\n",
       " ('sousse.n.01', 0.7465986013412476),\n",
       " ('cakchiquel.n.01', 0.7465184926986694),\n",
       " ('kota.n.01', 0.7464815974235535),\n",
       " ('grand_canal.n.02', 0.7463891506195068),\n",
       " ('kekchi.n.01', 0.745223879814148),\n",
       " ('aerie.n.02', 0.7440038323402405),\n",
       " (\"akwa'ala.n.01\", 0.7439688444137573),\n",
       " ('tarabulus.n.01', 0.7434621453285217),\n",
       " ('dobrich.n.01', 0.7422606945037842),\n",
       " ('homyel.n.01', 0.7406438589096069),\n",
       " ('mandara.n.01', 0.740449070930481),\n",
       " ('torreon.n.01', 0.7402486205101013),\n",
       " ('mukalla.n.01', 0.739410400390625)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_embeddings.neighbor(\"tamale.n.01\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma2concept(entry):\n",
    "    return lexicon.Concept(\n",
    "        lemma=entry[\"lemma\"],\n",
    "        singular=entry[\"singular\"],\n",
    "        plural=entry[\"plural\"],\n",
    "        article=entry[\"article\"],\n",
    "        generic=entry[\"generic\"],\n",
    "        taxonomic_phrase=entry[\"taxonomic_phrase\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_path = \"../data/things/things-lemmas-annotated.csv\"\n",
    "\n",
    "# triples_prompts = []\n",
    "\n",
    "# read in concepts\n",
    "concepts = defaultdict(lexicon.Concept)\n",
    "with open(lemma_path, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # concepts.append(lemma2concept(row))\n",
    "        if row[\"remove\"] != \"1\":\n",
    "            concepts[row[\"lemma\"]] = lemma2concept(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concepts['school bus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_senses = set()\n",
    "concepts_annotated_senses = defaultdict(set)\n",
    "with open(\"../data/things/things-senses-annotated.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        if row[\"sense\"] != \"-\":\n",
    "            senses = row['sense'].split(\"&\")\n",
    "            for sense in senses:\n",
    "                things_senses.add(sense)\n",
    "                concepts_annotated_senses[row[\"concept\"]].add(sense)\n",
    "\n",
    "concepts_annotated_senses = dict(concepts_annotated_senses)\n",
    "# concepts_annotated_senses = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "women's clothing womens_clothing.n.00\n",
      "school supply school_supply.n.00\n",
      "breakfast breakfast.n.00\n",
      "office supply office_supply.n.00\n"
     ]
    }
   ],
   "source": [
    "for concept, senses in concepts_annotated_senses.items():\n",
    "    for s in senses:\n",
    "        try:\n",
    "            sense_embeddings(s)\n",
    "        except:\n",
    "            print(concept, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_path = \"../data/things/things-triples.csv\"\n",
    "triples = utils.read_csv_dict(triple_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'breakfast', 'office supply', 'school supply', \"women's clothing\"}"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_universe = set()\n",
    "anchor_synsets = defaultdict(str)\n",
    "anchor_children = defaultdict(set)\n",
    "synset_anchors = defaultdict(str)\n",
    "nonsense = set()\n",
    "hypernyms = defaultdict(str)\n",
    "for triple in triples:\n",
    "    hypernym = triple[\"hypernym\"]\n",
    "    hyponym = triple[\"hyponym\"]\n",
    "    anchor = triple[\"anchor\"]\n",
    "    if triple[\"anchor-sense\"] == \"-\":\n",
    "        nonsense.add(anchor)\n",
    "        if anchor in [\n",
    "            \"breakfast\",\n",
    "            \"office supply\",\n",
    "            \"school supply\",\n",
    "            \"women's clothing\",\n",
    "        ]:\n",
    "            if anchor == \"breakfast\":\n",
    "                anchor_sense = \"breakfast.n.00\"\n",
    "            elif anchor == \"office supply\":\n",
    "                anchor_sense = \"office_supply.n.00\"\n",
    "            elif anchor == \"school supply\":\n",
    "                anchor_sense = \"school_supply.n.00\"\n",
    "            elif anchor == \"women's clothing\":\n",
    "                anchor_sense = \"womens_clothing.n.00\"\n",
    "        anchor_synsets[anchor] = anchor_sense\n",
    "        synset_anchors[anchor_sense] = anchor\n",
    "        hypernyms[hyponym] = anchor\n",
    "    else:\n",
    "        anchor_sense = triple[\"anchor-sense\"]\n",
    "        if anchor == \"headwear\":\n",
    "            # print(triple)\n",
    "            anchor_sense = \"headdress.n.01\"\n",
    "        elif anchor == \"toy\":\n",
    "            anchor_sense = \"plaything.n.01\"\n",
    "        elif anchor == \"protective clothing\":\n",
    "            anchor_sense = \"protective_covering.n.01\"\n",
    "        elif anchor == \"breakfast\":\n",
    "            anchor_sense = \"breakfast.n.00\"\n",
    "        elif anchor == \"office supply\":\n",
    "            anchor_sense = \"office_supply.n.00\"\n",
    "        elif anchor == \"school supply\":\n",
    "            anchor_sense = \"school_supply.n.00\"\n",
    "        elif anchor == \"women's clothing\":\n",
    "            anchor_sense = \"womens_clothing.n.00\"\n",
    "        if anchor_sense == \"none\":\n",
    "            print(triple)\n",
    "        anchor_synsets[anchor] = anchor_sense\n",
    "        synset_anchors[anchor_sense] = anchor\n",
    "        hypernyms[hyponym] = anchor\n",
    "    # concept_universe.add(hypernym)\n",
    "    concept_universe.add(hyponym)\n",
    "    # concept_universe.add(anchor)\n",
    "    anchor_children[anchor].add(hyponym)\n",
    "\n",
    "anchor_children = dict(anchor_children)\n",
    "anchor_synsets = dict(anchor_synsets)\n",
    "synset_anchors = dict(synset_anchors)\n",
    "hypernyms = dict(hypernyms)\n",
    "nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "NONSENSES = {\n",
    "    \"breakfast.n.00\": [\n",
    "        \"bacon.n.01\",\n",
    "        \"bagel.n.01\",\n",
    "        \"bread.n.01\",\n",
    "        \"breakfast.n.01\",\n",
    "        \"breakfast_food.n.01\",\n",
    "        \"cereal.n.03\",\n",
    "        \"cream_cheese.n.01\",\n",
    "        \"crepe.n.01\",\n",
    "        \"crescent_roll.n.01\",\n",
    "        \"doughnut.n.02\",\n",
    "        \"egg.n.02\",\n",
    "        \"french_fries.n.01\",\n",
    "        \"granola.n.01\",\n",
    "        \"grits.n.01\",\n",
    "        \"ham.n.01\",\n",
    "        \"hamburger.n.01\",\n",
    "        \"hash.n.01\",\n",
    "        \"honey.n.01\",\n",
    "        \"jam.n.01\",\n",
    "        \"maple_syrup.n.01\",\n",
    "        \"marmalade.n.01\",\n",
    "        \"muffin.n.01\",\n",
    "        \"oatmeal.n.01\",\n",
    "        \"omelet.n.01\",\n",
    "        \"pancake.n.01\",\n",
    "        \"pastry.n.02\",\n",
    "        \"peanut_butter.n.01\",\n",
    "        \"sandwich.n.01\",\n",
    "        \"sausage.n.01\",\n",
    "        \"scone.n.01\",\n",
    "        \"scrambled_eggs.n.01\",\n",
    "        \"syrup.n.01\",\n",
    "        \"toast.n.01\",\n",
    "        \"waffle.n.01\",\n",
    "        \"yogurt.n.01\",\n",
    "    ],\n",
    "    \"office_supply.n.00\": [\n",
    "        \"binder.n.03\",\n",
    "        \"clipboard.n.01\",\n",
    "        \"envelope.n.01\",\n",
    "        \"eraser.n.01\",\n",
    "        \"file.n.01\",\n",
    "        \"booklet.n.01\",\n",
    "        \"fountain_pen.n.01\",\n",
    "        \"glue.n.01\",\n",
    "        \"highlighter.n.01\",\n",
    "        \"ink.n.01\",\n",
    "        \"notebook.n.01\",\n",
    "        \"notepad.n.01\",\n",
    "        \"paper.n.01\",\n",
    "        \"paper_clip.n.01\",\n",
    "        \"pen.n.01\",\n",
    "        \"pencil.n.01\",\n",
    "        \"pencil_sharpener.n.01\",\n",
    "        \"punch.n.03\",\n",
    "        \"rubber_band.n.01\",\n",
    "        \"scissors.n.01\",\n",
    "        \"staple.n.05\",\n",
    "        \"stapler.n.01\",\n",
    "        \"tack.n.02\",\n",
    "        \"tape.n.01\",\n",
    "        \"thumbtack.n.01\",\n",
    "    ],\n",
    "    \"school_supply.n.00\": [\n",
    "        \"backpack.n.01\",\n",
    "        \"binder.n.03\",\n",
    "        \"book.n.01\",\n",
    "        \"calculator.n.02\",\n",
    "        \"chalk.n.04\",\n",
    "        \"blackboard.n.01\",\n",
    "        \"crayon.n.01\",\n",
    "        \"eraser.n.01\",\n",
    "        \"booklet.n.01\",\n",
    "        \"fountain_pen.n.01\",\n",
    "        \"glue.n.01\",\n",
    "        \"highlighter.n.01\",\n",
    "        \"ink.n.01\",\n",
    "        \"notebook.n.01\",\n",
    "        \"inkwell.n.01\",\n",
    "        \"marker.n.03\",\n",
    "        \"notepad.n.01\",\n",
    "        \"paper.n.01\",\n",
    "        \"paper_clip.n.01\",\n",
    "        \"pen.n.01\",\n",
    "        \"pencil.n.01\",\n",
    "        \"pencil_sharpener.n.01\",\n",
    "        \"scissors.n.01\",\n",
    "        \"staple.n.05\",\n",
    "        \"stapler.n.01\",\n",
    "    ],\n",
    "    \"womens_clothing.n.00\": [\n",
    "        \"bikini.n.02\",\n",
    "        \"blouse.n.01\",\n",
    "        \"feather_boa.n.01\",\n",
    "        \"brassiere.n.01\",\n",
    "        \"corset.n.01\",\n",
    "        \"dress.n.01\",\n",
    "        \"nylons.n.01\",\n",
    "        \"garter.n.01\",\n",
    "        \"headscarf.n.01\",\n",
    "        \"kimono.n.01\",\n",
    "        \"legging.n.01\",\n",
    "        \"lingerie.n.01\",\n",
    "        \"pants_suit.n.01\",\n",
    "        \"pantyhose.n.01\",\n",
    "        \"skirt.n.02\",\n",
    "        \"stocking.n.01\",\n",
    "        \"tiara.n.01\",\n",
    "        \"head_covering.n.01\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in anchor_synsets:\n",
    "    if a == 'none':\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things_senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space = []\n",
    "# for concept_universe in concept_universe - anchor_children['animal']:\n",
    "\n",
    "\n",
    "def synset_names(concept, return_map=False, both=False):\n",
    "    # synset_list = [synset.name() for synset in wn.synsets(concept, \"n\")]\n",
    "    synset_list = [s for s in concepts_annotated_senses[concept]]\n",
    "    if return_map:\n",
    "        synset_map = {s: concept for s in synset_list}\n",
    "        concept_map = {concept: s for s in synset_list}\n",
    "        return synset_map, concept_map\n",
    "    elif both:\n",
    "        return (\n",
    "            synset_list,\n",
    "            {s: concept for s in synset_list},\n",
    "            {concept: s for s in synset_list},\n",
    "        )\n",
    "    else:\n",
    "        return synset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synset_names(\"tupperware\")\n",
    "# sense_embeddings(\"bread-bin.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_map = {}\n",
    "concept_map = {}\n",
    "concept_synsets = defaultdict(set)\n",
    "for concept in concept_universe:\n",
    "    lst, smap, cmap = synset_names(concept, both=True)\n",
    "    synset_map.update(smap)\n",
    "    concept_map.update(cmap)\n",
    "    concept_synsets[concept].update(set(lst))\n",
    "    # synset_map.update(synset_names(concept, return_map=True))\n",
    "\n",
    "concept_synsets = dict(concept_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hypernym(synset, target):\n",
    "    try:\n",
    "        target = wn.synset(target)\n",
    "        for path in wn.synset(synset).hypernym_paths():\n",
    "            if target in path:\n",
    "                return True\n",
    "    except:\n",
    "        return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synsets = set()\n",
    "for k,v in concept_synsets.items():\n",
    "    for vv in v:\n",
    "        if vv in things_senses:\n",
    "            all_synsets.add(vv)\n",
    "    # all_synsets.update(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_universe = set(list(all_synsets) + list(anchor_synsets.values()))\n",
    "synset_universe = [s for s in synset_universe if s not in NONSENSES.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1508"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synset_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liquid_body_substance.n.01\n",
      "substance.n.01\n",
      "transparent_substance.n.01\n",
      "humic_substance.n.01\n",
      "body_substance.n.01\n",
      "substance_abuse.n.01\n",
      "controlled_substance.n.01\n",
      "substance.n.07\n",
      "solid_body_substance.n.01\n",
      "substance.n.04\n"
     ]
    }
   ],
   "source": [
    "# \"school_bus.n.01\" in all_synsets\n",
    "# sense_embeddings(\"school_bus.n.01\")\n",
    "\n",
    "# sense_embeddings(\"stirfry.n.01\")\n",
    "for w in sense_embeddings.vocab:\n",
    "    if \"substance\" in w:\n",
    "        print(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Reduced Sense Embs VectorSpaceModel: 1512 x 4096>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sense_embeddings(synset_universe)\n",
    "universe_vectors = sense_embeddings(synset_universe)\n",
    "\n",
    "excess = []\n",
    "excess_vocab = []\n",
    "for k, v in NONSENSES.items():\n",
    "    excess.append(sense_embeddings(v).mean(0))\n",
    "    excess_vocab.append(k)\n",
    "\n",
    "reduced_sense_embeddings = vsm.VectorSpaceModel(\"Reduced Sense Embs\")\n",
    "reduced_sense_embeddings.load_vectors_from_tensor(\n",
    "    torch.cat((universe_vectors, torch.stack(excess))), synset_universe + excess_vocab\n",
    ")\n",
    "\n",
    "reduced_sense_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('school_supply.n.00', 0.9904826879501343),\n",
       " ('highlighter.n.02', 0.9274128675460815),\n",
       " ('latch.n.02', 0.9172189235687256),\n",
       " ('dustpan.n.02', 0.9132122993469238),\n",
       " ('backscratcher.n.02', 0.9129491448402405),\n",
       " ('bolt.n.06', 0.912151575088501),\n",
       " ('whisk.n.01', 0.9098642468452454),\n",
       " ('stretcher.n.03', 0.9062033891677856),\n",
       " ('envelope.n.02', 0.9012345671653748),\n",
       " ('top.n.10', 0.9010876417160034)]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_sense_embeddings.neighbor(\"office_supply.n.00\",  k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_neighbors = defaultdict(list)\n",
    "for anchor, anchor_synset in anchor_synsets.items():\n",
    "    # leftover = set()\n",
    "    # anchor = synset_anchors[anchor_synset]\n",
    "    space = concept_universe - anchor_children[anchor]\n",
    "    space_synsets = set()\n",
    "    for c in space:\n",
    "        if c in concepts.keys():\n",
    "            for cs in concept_synsets[c]:\n",
    "                if cs not in anchor_synsets.values() and cs in synset_universe:\n",
    "                    space_synsets.add(cs)\n",
    "\n",
    "    space_synsets = list(space_synsets)\n",
    "    # filter out hypernymy cases\n",
    "    space_synsets = [s for s in space_synsets if is_hypernym(s, anchor_synset) == False]\n",
    "    # space_synsets_filtered = []\n",
    "    # for s in space_synsets:\n",
    "    #     if s in NONSENSES.keys():\n",
    "    #         pass\n",
    "    #     elif is_hypernym(s, anchor_synset) == False:\n",
    "    #         space_synsets_filtered.append(s)\n",
    "\n",
    "    neighbors = reduced_sense_embeddings.neighbor(\n",
    "        anchor_synset,\n",
    "        space=space_synsets,\n",
    "        k=len(anchor_children[anchor]),\n",
    "        names_only=True,\n",
    "        ignore_first=False,\n",
    "    )[0]\n",
    "    neighbor_concepts = [synset_map[n] for n in neighbors]\n",
    "    neighbor_concepts = list(OrderedSet(neighbor_concepts))[\n",
    "        : math.ceil(len(anchor_children[anchor]) / 2)\n",
    "    ]\n",
    "    anchor_neighbors[anchor].extend(neighbor_concepts)\n",
    "\n",
    "    # non-neighbors\n",
    "    non_neighbors = reduced_sense_embeddings.neighbor(\n",
    "        anchor_synset,\n",
    "        space=space_synsets,\n",
    "        k=len(anchor_children[anchor]),\n",
    "        names_only=True,\n",
    "        ignore_first=False,\n",
    "        nearest=False,\n",
    "    )[0]\n",
    "    non_neighbor_concepts = [synset_map[n] for n in non_neighbors]\n",
    "    non_neighbor_concepts = list(OrderedSet(non_neighbor_concepts))[\n",
    "        : math.ceil(len(anchor_children[anchor]) / 2)\n",
    "    ]\n",
    "    anchor_neighbors[anchor].extend(non_neighbor_concepts)\n",
    "\n",
    "anchor_neighbors = dict(anchor_neighbors)\n",
    "random.seed(42)\n",
    "anchor_neighbors = {k: random.sample(v, len(v)) for k, v in anchor_neighbors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any negatively sampled concept is a children (shouldn't be)\n",
    "for anchor, neighbors in anchor_neighbors.items():\n",
    "    for n in neighbors:\n",
    "        if n in anchor_children[anchor]:\n",
    "            print(anchor, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57508"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just generate the negatives sample triples\n",
    "44 * 1307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concept_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(v) for k,v in anchor_children.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(v) for k,v in anchor_neighbors.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'animal.n.01'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_synsets['animal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sample_triples = []\n",
    "for anchor, negative_samples in anchor_neighbors.items():\n",
    "    for ns in negative_samples:\n",
    "        negative_sample_triples.append((anchor, anchor_synsets[anchor], ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_sample_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concepts['garden tool']\n",
    "# concepts['crib']\n",
    "'crib' in concepts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/things/things-sense_based_ns-triples.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"anchor\", \"anchor-sense\", \"hyponym\"])\n",
    "    for triple in negative_sample_triples:\n",
    "        writer.writerow(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bird'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_map['bird.n.01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save anchor, every concept similarities\n",
    "anchor_concept_sims = defaultdict(list)\n",
    "for anchor, anchor_synset in anchor_synsets.items():\n",
    "    space = concept_universe\n",
    "    space_synsets = set()\n",
    "    for c in space:\n",
    "        if c in concepts.keys():\n",
    "            for cs in concept_synsets[c]:\n",
    "                if cs not in anchor_synsets.values() and cs in synset_universe:\n",
    "                    space_synsets.add(cs)\n",
    "    neighbor_sims = reduced_sense_embeddings.neighbor(anchor_synset, k = len(space_synsets), space=space_synsets, ignore_first=False)\n",
    "    for concept_sense, sim in neighbor_sims[0]:\n",
    "        try:\n",
    "            concept = synset_map[concept_sense]\n",
    "        except:\n",
    "            synset_anchors[concept_sense]\n",
    "        anchor_concept_sims[(anchor, concept)].append(sim)\n",
    "\n",
    "anchor_concept_sims_csv = []\n",
    "for (anchor, concept), sims in anchor_concept_sims.items():\n",
    "    anchor_concept_sims_csv.append((anchor, concept, max(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45496"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchor_concept_sims_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/things/things-sense_based_anchor_sims.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"anchor\", \"concept\", \"similarity\"])\n",
    "    for row in anchor_concept_sims_csv:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home decor 45 46\n",
      "insect 17 18\n",
      "plant 47 48\n",
      "animal 177 178\n",
      "toiletry 31 32\n",
      "container 105 106\n",
      "kitchen tool 27 28\n",
      "musical instrument 33 34\n",
      "breakfast 35 36\n",
      "headwear 19 20\n",
      "medical equipment 27 28\n",
      "accessory 37 38\n",
      "condiment 15 16\n",
      "jewelry 15 16\n",
      "footwear 15 16\n",
      "office supply 25 26\n",
      "drink 19 20\n",
      "water vehicle 19 20\n",
      "bird 27 28\n",
      "game 19 20\n",
      "dessert 37 38\n",
      "garden tool 17 18\n"
     ]
    }
   ],
   "source": [
    "for anchor, synset in anchor_synsets.items():\n",
    "    if len(anchor_children[anchor]) != len(anchor_neighbors[anchor]):\n",
    "        print(anchor, len(anchor_children[anchor]), len(anchor_neighbors[anchor]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmisra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
